import streamlit as st
from google import genai
from google.genai import types
from pypdf import PdfReader
import os
import gdown

# Configura√ß√£o da P√°gina
st.set_page_config(
    page_title="Harvard Mentor AI",
    page_icon="üéì",
    layout="wide"
)

# --- 1. CONFIGURA√á√ÉO DE SEGREDOS ---
api_key = st.secrets.get("GOOGLE_API_KEY")
file_id = st.secrets.get("GDRIVE_FILE_ID")

# --- 2. FUN√á√ïES DE INFRAESTRUTURA ---

def download_pdf_if_needed(filename):
    """
    Verifica se o PDF existe localmente. 
    Se n√£o existir (cen√°rio do Streamlit Cloud), baixa do Google Drive.
    """
    if os.path.exists(filename):
        return True
    
    if not file_id:
        st.error("Erro: ID do arquivo n√£o configurado nos Secrets.")
        return False

    with st.spinner("Baixando material de estudo seguro (Isso acontece apenas uma vez)..."):
        try:
            
            url = f'https://drive.google.com/uc?id={file_id}'
            gdown.download(url, filename, quiet=False)
            return True
        except Exception as e:
            st.error(f"Falha ao baixar o arquivo: {e}")
            return False

@st.cache_resource
def load_pdf_text(pdf_path):
    """L√™ o PDF e extrai o texto. Usa cache para performance."""
    if not download_pdf_if_needed(pdf_path):
        return None
    
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"Erro ao ler PDF: {e}")
        return None

def get_gemini_response(history, mode, context_text):
    # Defini√ß√£o das Personas (System Prompts)
    prompts = {
        "Consultor": f"""
            Voc√™ √© um Consultor S√™nior da Harvard Business School.
            CONTEXTO: O usu√°rio tem um desafio de neg√≥cios.
            BASE DE CONHECIMENTO: Use EXCLUSIVAMENTE o seguinte material: {context_text}
            
            SUA MISS√ÉO:
            1. Analise o problema do usu√°rio.
            2. Encontre os frameworks/conceitos no material que se aplicam.
            3. D√™ uma resposta estruturada (Diagn√≥stico -> Conceito -> Plano de A√ß√£o).
            4. Cite o m√≥dulo de onde tirou a informa√ß√£o.
            """,
        
        "Quiz": f"""
            Voc√™ √© um Professor avaliador.
            BASE DE CONHECIMENTO: {context_text}
            
            SUA MISS√ÉO:
            1. Gere UMA pergunta de m√∫ltipla escolha ou discursiva baseada no texto.
            2. Aguarde a resposta do usu√°rio.
            3. Se ele acertar, parabenize e explique o conceito. Se errar, corrija gentilmente citando o texto.
            4. Mantenha o tom educativo e desafiador.
            """,
        
        "Roleplay": f"""
            ATEN√á√ÉO: Ignore que voc√™ √© uma IA. Voc√™ √© agora um PERSONAGEM.
            CEN√ÅRIO: Simula√ß√£o de Negocia√ß√£o/Lideran√ßa baseada em: {context_text}
            
            SUA MISS√ÉO:
            1. Aja como uma contraparte dif√≠cil (ex: cliente irritado, chefe exigente).
            2. Reaja √†s falas do usu√°rio. Se ele usar boas t√©cnicas do texto, ceda um pouco. Se ele for ruim, seja duro.
            3. NUNCA saia do personagem, a menos que o usu√°rio diga "FIM DA SIMULA√á√ÉO".
            """
    }

    system_instruction = prompts.get(mode, "Voc√™ √© um assistente √∫til.")

    # 2. Inicializa o Cliente (Nova Sintaxe)
    client = genai.Client(api_key=api_key)

    # 3. Converte hist√≥rico do Streamlit para o formato da Google
    # Streamlit usa: {"role": "user/assistant", "content": "texto"}
    # Google GenAI usa: types.Content(role="user/model", parts=[...])
    
    contents = []
    for msg in chat_history_streamlit:
        role = "user" if msg["role"] == "user" else "model"
        contents.append(
            types.Content(
                role=role,
                parts=[types.Part.from_text(text=msg["content"])]
            )
        )

    # 4. Configura√ß√£o da Gera√ß√£o
    generate_content_config = types.GenerateContentConfig(
        temperature=0.7,
        top_p=0.95,
        max_output_tokens=2000,
        system_instruction=system_instruction,
    )

    # 5. Chamada ao Modelo
    try:
        response = client.models.generate_content(
            model="gemini-1.5-flash",
            contents=contents,
            config=generate_content_config
        )
        return response.text
    except Exception as e:
        return f"Erro na API Google: {str(e)}"

# --- 3. INTERFACE (FRONTEND) ---

st.sidebar.title("Harvard Impact AI")
page = st.sidebar.radio("Menu", ["Introdu√ß√£o", "Mentor Virtual"])

if page == "Introdu√ß√£o":
    st.title("Domine os Fundamentos de Neg√≥cios üöÄ")
    st.markdown("""
    Bem-vindo ao seu Mentor de Neg√≥cios baseado no curr√≠culo de Harvard.
    Utilizando a tecnologia **Google Gemini 1.5 Flash**.
    
    Escolha seu modo no menu lateral:
    1.  **Consultor:** Resolu√ß√£o de problemas.
    2.  **Quiz:** Estudo ativo.
    3.  **Roleplay:** Simula√ß√£o pr√°tica.
    """)

elif page == "Mentor Virtual":
    if not api_key:
        st.warning("‚ö†Ô∏è API Key n√£o detectada nos Secrets.")
        st.stop()
    
    pdf_filename = "Harvard Manager Mentor.pdf"
    pdf_text = load_pdf_text(pdf_filename)
    
    if not pdf_text:
        st.stop()

    col1, col2 = st.columns([3, 1])
    with col1:
        mode = st.radio("Modo:", ["Consultor", "Quiz", "Roleplay"], horizontal=True)
    with col2:
        if st.button("Limpar Chat üóëÔ∏è"):
            st.session_state.messages = []
            st.rerun()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        avatar = "ü§ñ" if message["role"] == "assistant" else "üë§"
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])

    if prompt := st.chat_input("Digite sua mensagem..."):
        # Adiciona mensagem do usu√°rio ao hist√≥rico visual
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user", avatar="üë§"):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar="ü§ñ"):
            with st.spinner("Analisando..."):
                # Passa o hist√≥rico completo + nova mensagem (j√° inclusa no state)
                response_text = get_gemini_response(st.session_state.messages, mode, pdf_text)
                
                st.markdown(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})
