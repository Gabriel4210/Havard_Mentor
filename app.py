import streamlit as st
import google.generativeai as genai
from pypdf import PdfReader
import os

# Configura√ß√£o da P√°gina
st.set_page_config(page_title="Harvard Mentor AI", layout="wide")

# --- 1. CONFIGURA√á√ÉO E SEGURAN√áA ---
# Em produ√ß√£o (Streamlit Cloud), pegaremos a chave dos "Secrets". 
# Localmente, voc√™ pode usar uma vari√°vel de ambiente ou input manual.
api_key = st.secrets.get("GOOGLE_API_KEY")

# Se n√£o tiver chave configurada, pede na tela (bom para testar)
if not api_key:
    api_key = st.sidebar.text_input("Insira sua Google API Key", type="password")

# --- 2. FUN√á√ïES DE BACKEND ---

@st.cache_resource
def load_pdf_text(pdf_path):
    """L√™ o PDF e extrai o texto. Usa cache para n√£o reler a cada clique."""
    if not os.path.exists(pdf_path):
        return None
    
    reader = PdfReader(pdf_path)
    text = ""
    # Itera pelas p√°ginas extraindo texto
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

def get_gemini_response(history, mode, context_text):
    """Envia o hist√≥rico e o contexto para o Gemini."""
    
    # Define a 'Persona' baseada no modo escolhido
    if mode == "Consultor":
        system_instruction = f"""
        Voc√™ √© um Consultor S√™nior formado com o material do 'Harvard Manager Mentor'.
        SEU OBJETIVO: Ajudar o usu√°rio a resolver problemas pr√°ticos de neg√≥cios usando APENAS os conceitos do material fornecido.
        MATERIAL DE BASE: {context_text}
        DIRETRIZES:
        - Seja direto e profissional.
        - Cite o conceito espec√≠fico (ex: 'Segundo o m√≥dulo de Negocia√ß√£o...').
        - D√™ planos de a√ß√£o (Passo 1, Passo 2).
        """
    elif mode == "Quiz":
        system_instruction = f"""
        Voc√™ √© um Examinador Rigoroso da Harvard.
        SEU OBJETIVO: Testar o conhecimento do usu√°rio sobre o material.
        MATERIAL DE BASE: {context_text}
        DIRETRIZES:
        - Fa√ßa UMA pergunta por vez baseada no texto.
        - Espere a resposta.
        - Avalie se est√° certo ou errado e explique o porqu√™ baseando-se no texto.
        - Depois, proponha outra pergunta.
        """
    elif mode == "Roleplay":
        system_instruction = f"""
        Voc√™ √© um ator em uma simula√ß√£o de neg√≥cios.
        SEU OBJETIVO: Agir como uma contraparte dif√≠cil (um cliente bravo, um funcion√°rio desmotivado ou um fornecedor r√≠gido).
        MATERIAL DE BASE: {context_text}
        DIRETRIZES:
        - N√£o saia do personagem.
        - O usu√°rio deve tentar aplicar as t√©cnicas do curso para lidar com voc√™.
        - No final, se o usu√°rio pedir 'Feedback', saia do personagem e avalie a performance dele baseada no curso.
        """
    else:
        system_instruction = "Voc√™ √© um assistente √∫til."

    # Configura o modelo
    genai.configure(api_key=api_key)
    # Usamos o flash por ser r√°pido e ter contexto longo
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        system_instruction=system_instruction
    )
    
    # Inicia o chat com o hist√≥rico
    chat = model.start_chat(history=history)
    
    # Pega a √∫ltima mensagem do usu√°rio (que j√° foi adicionada ao hist√≥rico visual, mas precisa ser enviada ao modelo)
    # Nota: A lib do Google gerencia o hist√≥rico internamente no objeto 'chat', 
    # mas aqui vamos enviar a mensagem atual para obter a resposta.
    response = chat.send_message(st.session_state.messages[-1]["content"])
    
    return response.text

# --- 3. INTERFACE DO USU√ÅRIO ---

# Sidebar de Navega√ß√£o e Modos
st.sidebar.title("üéì Harvard Mentor AI")
page = st.sidebar.radio("Navega√ß√£o", ["Introdu√ß√£o", "Chat com Mentor"])

if page == "Introdu√ß√£o":
    st.title("Bem-vindo ao Harvard Mentor AI üöÄ")
    st.markdown("""
    Este projeto aplica conhecimentos de **Marketing, Finan√ßas, Negocia√ß√£o e Lideran√ßa** baseados no curr√≠culo *Harvard Business Impact*.
    
    ### O que este app faz?
    Ele transforma o conte√∫do est√°tico das aulas em um mentor interativo utilizando **Intelig√™ncia Artificial (Google Gemini 1.5)**.
    
    ### Como usar?
    V√° para a aba **Chat com Mentor** e escolha um modo:
    1.   **Consultor:** Traga um problema real e receba conselhos baseados na teoria.
    2.   **Quiz:** Teste seus conhecimentos. O Mentor far√° perguntas sobre o material.
    3.   **Roleplay:** Simule situa√ß√µes dif√≠ceis (negocia√ß√µes, conflitos) e treine sua resposta.
    
    ---
    *Disclaimer: Este √© um projeto educacional de portf√≥lio. O conte√∫do base pertence √† Harvard Business School Publishing.*
    """)

elif page == "Chat com Mentor":
    # Carregar o PDF (apenas uma vez)
    # IMPORTANTE: O nome do arquivo deve ser exato
    pdf_text = load_pdf_text("Harvard Manager Mentor.pdf")
    
    if not pdf_text:
        st.error("Erro: Arquivo PDF n√£o encontrado. Verifique se o arquivo 'Harvard Manager Mentor.pdf' est√° na raiz do projeto.")
        st.stop()

    if not api_key:
        st.warning("Por favor, insira a API Key na barra lateral para come√ßar.")
        st.stop()

    # Seletor de Modo
    mode = st.radio("Escolha o Modo de Intera√ß√£o:", ["Consultor", "Quiz", "Roleplay"], horizontal=True)
    
    # Bot√£o para limpar conversa se trocar de modo
    if st.button("Reiniciar Conversa"):
        st.session_state.messages = []
        st.rerun()

    # Inicializa o hist√≥rico visual
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Exibe as mensagens anteriores na tela
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Input do Usu√°rio
    if prompt := st.chat_input("Digite sua mensagem..."):
        # 1. Adiciona msg do usu√°rio ao visual
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. Gera resposta
        with st.chat_message("assistant"):
            with st.spinner("O Mentor est√° analisando o material..."):
                try:
                    # Converte hist√≥rico do Streamlit para o formato do Google (opcional, mas o .start_chat lida bem)
                    # Aqui simplificamos enviando o contexto no system prompt a cada chamada ou mantendo sess√£o
                    # O Gemini 1.5 √© stateless via API REST, mas a lib python mant√©m estado se usarmos chat.history.
                    # Para simplificar o c√≥digo no Streamlit (que recarrega tudo), recriamos a chamada.
                    
                    history_gemini = [
                        {"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]}
                        for m in st.session_state.messages[:-1] # Pega tudo menos a atual que ser√° enviada
                    ]
                    
                    response_text = get_gemini_response(history_gemini, mode, pdf_text)
                    st.markdown(response_text)
                    
                    # 3. Adiciona resposta ao hist√≥rico visual
                    st.session_state.messages.append({"role": "assistant", "content": response_text})
                    
                except Exception as e:
                    st.error(f"Ocorreu um erro na API: {e}")
